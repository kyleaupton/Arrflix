// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.30.0
// source: import_tasks.sql

package dbgen

import (
	"context"
	"time"

	"github.com/jackc/pgx/v5/pgtype"
)

const cancelImportTask = `-- name: CancelImportTask :one
UPDATE import_task
SET status = 'cancelled',
    updated_at = now()
WHERE id = $1
  AND status = 'pending'
RETURNING id, status, download_job_id, source_path, previous_task_id, media_type, media_item_id, episode_id, library_id, name_template_id, dest_path, import_method, media_file_id, attempt_count, max_attempts, next_run_at, last_error, error_category, created_at, updated_at
`

func (q *Queries) CancelImportTask(ctx context.Context, id pgtype.UUID) (ImportTask, error) {
	row := q.db.QueryRow(ctx, cancelImportTask, id)
	var i ImportTask
	err := row.Scan(
		&i.ID,
		&i.Status,
		&i.DownloadJobID,
		&i.SourcePath,
		&i.PreviousTaskID,
		&i.MediaType,
		&i.MediaItemID,
		&i.EpisodeID,
		&i.LibraryID,
		&i.NameTemplateID,
		&i.DestPath,
		&i.ImportMethod,
		&i.MediaFileID,
		&i.AttemptCount,
		&i.MaxAttempts,
		&i.NextRunAt,
		&i.LastError,
		&i.ErrorCategory,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const cancelPendingImportTasksForJob = `-- name: CancelPendingImportTasksForJob :exec
UPDATE import_task
SET status = 'cancelled',
    updated_at = now()
WHERE download_job_id = $1
  AND status = 'pending'
`

// Cancel all pending import tasks for a download job
func (q *Queries) CancelPendingImportTasksForJob(ctx context.Context, downloadJobID pgtype.UUID) error {
	_, err := q.db.Exec(ctx, cancelPendingImportTasksForJob, downloadJobID)
	return err
}

const claimRunnableImportTasks = `-- name: ClaimRunnableImportTasks :many
WITH cte AS (
  SELECT id
  FROM import_task
  WHERE status = 'pending'
    AND next_run_at <= now()
  ORDER BY next_run_at ASC
  FOR UPDATE SKIP LOCKED
  LIMIT $1
)
UPDATE import_task t
SET status = 'in_progress',
    updated_at = now()
FROM cte
WHERE t.id = cte.id
RETURNING t.id, t.status, t.download_job_id, t.source_path, t.previous_task_id, t.media_type, t.media_item_id, t.episode_id, t.library_id, t.name_template_id, t.dest_path, t.import_method, t.media_file_id, t.attempt_count, t.max_attempts, t.next_run_at, t.last_error, t.error_category, t.created_at, t.updated_at
`

// Claims tasks that are ready to be processed (pending only, not in_progress)
// Uses FOR UPDATE SKIP LOCKED to prevent duplicate processing
func (q *Queries) ClaimRunnableImportTasks(ctx context.Context, limit int32) ([]ImportTask, error) {
	rows, err := q.db.Query(ctx, claimRunnableImportTasks, limit)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []ImportTask
	for rows.Next() {
		var i ImportTask
		if err := rows.Scan(
			&i.ID,
			&i.Status,
			&i.DownloadJobID,
			&i.SourcePath,
			&i.PreviousTaskID,
			&i.MediaType,
			&i.MediaItemID,
			&i.EpisodeID,
			&i.LibraryID,
			&i.NameTemplateID,
			&i.DestPath,
			&i.ImportMethod,
			&i.MediaFileID,
			&i.AttemptCount,
			&i.MaxAttempts,
			&i.NextRunAt,
			&i.LastError,
			&i.ErrorCategory,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const countImportTasksByStatus = `-- name: CountImportTasksByStatus :one
SELECT
  COUNT(*) FILTER (WHERE status = 'pending')::int AS pending,
  COUNT(*) FILTER (WHERE status = 'in_progress')::int AS in_progress,
  COUNT(*) FILTER (WHERE status = 'completed')::int AS completed,
  COUNT(*) FILTER (WHERE status = 'failed')::int AS failed,
  COUNT(*) FILTER (WHERE status = 'cancelled')::int AS cancelled
FROM import_task
`

type CountImportTasksByStatusRow struct {
	Pending    int32 `json:"pending"`
	InProgress int32 `json:"in_progress"`
	Completed  int32 `json:"completed"`
	Failed     int32 `json:"failed"`
	Cancelled  int32 `json:"cancelled"`
}

func (q *Queries) CountImportTasksByStatus(ctx context.Context) (CountImportTasksByStatusRow, error) {
	row := q.db.QueryRow(ctx, countImportTasksByStatus)
	var i CountImportTasksByStatusRow
	err := row.Scan(
		&i.Pending,
		&i.InProgress,
		&i.Completed,
		&i.Failed,
		&i.Cancelled,
	)
	return i, err
}

const createImportTask = `-- name: CreateImportTask :one

INSERT INTO import_task (
  status,
  download_job_id,
  source_path,
  previous_task_id,
  media_type,
  media_item_id,
  episode_id,
  library_id,
  name_template_id
)
VALUES (
  'pending',
  $1,
  $2,
  $3,
  $4,
  $5,
  $6,
  $7,
  $8
)
RETURNING id, status, download_job_id, source_path, previous_task_id, media_type, media_item_id, episode_id, library_id, name_template_id, dest_path, import_method, media_file_id, attempt_count, max_attempts, next_run_at, last_error, error_category, created_at, updated_at
`

type CreateImportTaskParams struct {
	DownloadJobID  pgtype.UUID `json:"download_job_id"`
	SourcePath     string      `json:"source_path"`
	PreviousTaskID pgtype.UUID `json:"previous_task_id"`
	MediaType      string      `json:"media_type"`
	MediaItemID    pgtype.UUID `json:"media_item_id"`
	EpisodeID      pgtype.UUID `json:"episode_id"`
	LibraryID      pgtype.UUID `json:"library_id"`
	NameTemplateID pgtype.UUID `json:"name_template_id"`
}

// Import tasks (per-file import tracking with reimport chain)
func (q *Queries) CreateImportTask(ctx context.Context, arg CreateImportTaskParams) (ImportTask, error) {
	row := q.db.QueryRow(ctx, createImportTask,
		arg.DownloadJobID,
		arg.SourcePath,
		arg.PreviousTaskID,
		arg.MediaType,
		arg.MediaItemID,
		arg.EpisodeID,
		arg.LibraryID,
		arg.NameTemplateID,
	)
	var i ImportTask
	err := row.Scan(
		&i.ID,
		&i.Status,
		&i.DownloadJobID,
		&i.SourcePath,
		&i.PreviousTaskID,
		&i.MediaType,
		&i.MediaItemID,
		&i.EpisodeID,
		&i.LibraryID,
		&i.NameTemplateID,
		&i.DestPath,
		&i.ImportMethod,
		&i.MediaFileID,
		&i.AttemptCount,
		&i.MaxAttempts,
		&i.NextRunAt,
		&i.LastError,
		&i.ErrorCategory,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const getImportTask = `-- name: GetImportTask :one
SELECT id, status, download_job_id, source_path, previous_task_id, media_type, media_item_id, episode_id, library_id, name_template_id, dest_path, import_method, media_file_id, attempt_count, max_attempts, next_run_at, last_error, error_category, created_at, updated_at FROM import_task
WHERE id = $1
`

func (q *Queries) GetImportTask(ctx context.Context, id pgtype.UUID) (ImportTask, error) {
	row := q.db.QueryRow(ctx, getImportTask, id)
	var i ImportTask
	err := row.Scan(
		&i.ID,
		&i.Status,
		&i.DownloadJobID,
		&i.SourcePath,
		&i.PreviousTaskID,
		&i.MediaType,
		&i.MediaItemID,
		&i.EpisodeID,
		&i.LibraryID,
		&i.NameTemplateID,
		&i.DestPath,
		&i.ImportMethod,
		&i.MediaFileID,
		&i.AttemptCount,
		&i.MaxAttempts,
		&i.NextRunAt,
		&i.LastError,
		&i.ErrorCategory,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const getImportTaskHistory = `-- name: GetImportTaskHistory :many
WITH RECURSIVE task_chain AS (
  -- Start with the given task
  SELECT it.id, it.status, it.download_job_id, it.source_path, it.previous_task_id, it.media_type, it.media_item_id, it.episode_id, it.library_id, it.name_template_id, it.dest_path, it.import_method, it.media_file_id, it.attempt_count, it.max_attempts, it.next_run_at, it.last_error, it.error_category, it.created_at, it.updated_at, 0 AS chain_depth
  FROM import_task it
  WHERE it.id = $1

  UNION ALL

  -- Follow previous_task_id links
  SELECT prev.id, prev.status, prev.download_job_id, prev.source_path, prev.previous_task_id, prev.media_type, prev.media_item_id, prev.episode_id, prev.library_id, prev.name_template_id, prev.dest_path, prev.import_method, prev.media_file_id, prev.attempt_count, prev.max_attempts, prev.next_run_at, prev.last_error, prev.error_category, prev.created_at, prev.updated_at, tc.chain_depth + 1
  FROM import_task prev
  JOIN task_chain tc ON tc.previous_task_id = prev.id
  WHERE tc.chain_depth < 50  -- Safety limit
)
SELECT id, status, download_job_id, source_path, previous_task_id, media_type, media_item_id, episode_id, library_id, name_template_id, dest_path, import_method, media_file_id, attempt_count, max_attempts, next_run_at, last_error, error_category, created_at, updated_at, chain_depth FROM task_chain
ORDER BY chain_depth ASC
`

type GetImportTaskHistoryRow struct {
	ID             pgtype.UUID `json:"id"`
	Status         string      `json:"status"`
	DownloadJobID  pgtype.UUID `json:"download_job_id"`
	SourcePath     string      `json:"source_path"`
	PreviousTaskID pgtype.UUID `json:"previous_task_id"`
	MediaType      string      `json:"media_type"`
	MediaItemID    pgtype.UUID `json:"media_item_id"`
	EpisodeID      pgtype.UUID `json:"episode_id"`
	LibraryID      pgtype.UUID `json:"library_id"`
	NameTemplateID pgtype.UUID `json:"name_template_id"`
	DestPath       *string     `json:"dest_path"`
	ImportMethod   *string     `json:"import_method"`
	MediaFileID    pgtype.UUID `json:"media_file_id"`
	AttemptCount   int32       `json:"attempt_count"`
	MaxAttempts    int32       `json:"max_attempts"`
	NextRunAt      time.Time   `json:"next_run_at"`
	LastError      *string     `json:"last_error"`
	ErrorCategory  *string     `json:"error_category"`
	CreatedAt      time.Time   `json:"created_at"`
	UpdatedAt      time.Time   `json:"updated_at"`
	ChainDepth     int32       `json:"chain_depth"`
}

// Get reimport chain for a task (follows previous_task_id links)
func (q *Queries) GetImportTaskHistory(ctx context.Context, id pgtype.UUID) ([]GetImportTaskHistoryRow, error) {
	rows, err := q.db.Query(ctx, getImportTaskHistory, id)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []GetImportTaskHistoryRow
	for rows.Next() {
		var i GetImportTaskHistoryRow
		if err := rows.Scan(
			&i.ID,
			&i.Status,
			&i.DownloadJobID,
			&i.SourcePath,
			&i.PreviousTaskID,
			&i.MediaType,
			&i.MediaItemID,
			&i.EpisodeID,
			&i.LibraryID,
			&i.NameTemplateID,
			&i.DestPath,
			&i.ImportMethod,
			&i.MediaFileID,
			&i.AttemptCount,
			&i.MaxAttempts,
			&i.NextRunAt,
			&i.LastError,
			&i.ErrorCategory,
			&i.CreatedAt,
			&i.UpdatedAt,
			&i.ChainDepth,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const getImportTaskWithDetails = `-- name: GetImportTaskWithDetails :one
SELECT
  it.id, it.status, it.download_job_id, it.source_path, it.previous_task_id, it.media_type, it.media_item_id, it.episode_id, it.library_id, it.name_template_id, it.dest_path, it.import_method, it.media_file_id, it.attempt_count, it.max_attempts, it.next_run_at, it.last_error, it.error_category, it.created_at, it.updated_at,
  mi.title AS media_title,
  mi.year AS media_year,
  mi.tmdb_id AS media_tmdb_id,
  mi.type AS media_item_type,
  me.episode_number,
  me.title AS episode_title,
  ms.season_number,
  l.name AS library_name,
  l.root_path AS library_root_path,
  nt.template AS name_template,
  nt.movie_dir_template,
  nt.series_show_template,
  nt.series_season_template,
  dj.candidate_title
FROM import_task it
JOIN media_item mi ON mi.id = it.media_item_id
LEFT JOIN media_episode me ON me.id = it.episode_id
LEFT JOIN media_season ms ON ms.id = me.season_id
JOIN library l ON l.id = it.library_id
JOIN name_template nt ON nt.id = it.name_template_id
LEFT JOIN download_job dj ON dj.id = it.download_job_id
WHERE it.id = $1
`

type GetImportTaskWithDetailsRow struct {
	ID                   pgtype.UUID `json:"id"`
	Status               string      `json:"status"`
	DownloadJobID        pgtype.UUID `json:"download_job_id"`
	SourcePath           string      `json:"source_path"`
	PreviousTaskID       pgtype.UUID `json:"previous_task_id"`
	MediaType            string      `json:"media_type"`
	MediaItemID          pgtype.UUID `json:"media_item_id"`
	EpisodeID            pgtype.UUID `json:"episode_id"`
	LibraryID            pgtype.UUID `json:"library_id"`
	NameTemplateID       pgtype.UUID `json:"name_template_id"`
	DestPath             *string     `json:"dest_path"`
	ImportMethod         *string     `json:"import_method"`
	MediaFileID          pgtype.UUID `json:"media_file_id"`
	AttemptCount         int32       `json:"attempt_count"`
	MaxAttempts          int32       `json:"max_attempts"`
	NextRunAt            time.Time   `json:"next_run_at"`
	LastError            *string     `json:"last_error"`
	ErrorCategory        *string     `json:"error_category"`
	CreatedAt            time.Time   `json:"created_at"`
	UpdatedAt            time.Time   `json:"updated_at"`
	MediaTitle           string      `json:"media_title"`
	MediaYear            *int32      `json:"media_year"`
	MediaTmdbID          *int64      `json:"media_tmdb_id"`
	MediaItemType        string      `json:"media_item_type"`
	EpisodeNumber        *int32      `json:"episode_number"`
	EpisodeTitle         *string     `json:"episode_title"`
	SeasonNumber         *int32      `json:"season_number"`
	LibraryName          string      `json:"library_name"`
	LibraryRootPath      string      `json:"library_root_path"`
	NameTemplate         string      `json:"name_template"`
	MovieDirTemplate     *string     `json:"movie_dir_template"`
	SeriesShowTemplate   *string     `json:"series_show_template"`
	SeriesSeasonTemplate *string     `json:"series_season_template"`
	CandidateTitle       *string     `json:"candidate_title"`
}

// Get import task with related media info and name template
func (q *Queries) GetImportTaskWithDetails(ctx context.Context, id pgtype.UUID) (GetImportTaskWithDetailsRow, error) {
	row := q.db.QueryRow(ctx, getImportTaskWithDetails, id)
	var i GetImportTaskWithDetailsRow
	err := row.Scan(
		&i.ID,
		&i.Status,
		&i.DownloadJobID,
		&i.SourcePath,
		&i.PreviousTaskID,
		&i.MediaType,
		&i.MediaItemID,
		&i.EpisodeID,
		&i.LibraryID,
		&i.NameTemplateID,
		&i.DestPath,
		&i.ImportMethod,
		&i.MediaFileID,
		&i.AttemptCount,
		&i.MaxAttempts,
		&i.NextRunAt,
		&i.LastError,
		&i.ErrorCategory,
		&i.CreatedAt,
		&i.UpdatedAt,
		&i.MediaTitle,
		&i.MediaYear,
		&i.MediaTmdbID,
		&i.MediaItemType,
		&i.EpisodeNumber,
		&i.EpisodeTitle,
		&i.SeasonNumber,
		&i.LibraryName,
		&i.LibraryRootPath,
		&i.NameTemplate,
		&i.MovieDirTemplate,
		&i.SeriesShowTemplate,
		&i.SeriesSeasonTemplate,
		&i.CandidateTitle,
	)
	return i, err
}

const listImportTasks = `-- name: ListImportTasks :many
SELECT id, status, download_job_id, source_path, previous_task_id, media_type, media_item_id, episode_id, library_id, name_template_id, dest_path, import_method, media_file_id, attempt_count, max_attempts, next_run_at, last_error, error_category, created_at, updated_at FROM import_task
ORDER BY created_at DESC
LIMIT $2
OFFSET $1
`

type ListImportTasksParams struct {
	OffsetVal int32 `json:"offset_val"`
	LimitVal  int32 `json:"limit_val"`
}

func (q *Queries) ListImportTasks(ctx context.Context, arg ListImportTasksParams) ([]ImportTask, error) {
	rows, err := q.db.Query(ctx, listImportTasks, arg.OffsetVal, arg.LimitVal)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []ImportTask
	for rows.Next() {
		var i ImportTask
		if err := rows.Scan(
			&i.ID,
			&i.Status,
			&i.DownloadJobID,
			&i.SourcePath,
			&i.PreviousTaskID,
			&i.MediaType,
			&i.MediaItemID,
			&i.EpisodeID,
			&i.LibraryID,
			&i.NameTemplateID,
			&i.DestPath,
			&i.ImportMethod,
			&i.MediaFileID,
			&i.AttemptCount,
			&i.MaxAttempts,
			&i.NextRunAt,
			&i.LastError,
			&i.ErrorCategory,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const listImportTasksByDownloadJob = `-- name: ListImportTasksByDownloadJob :many
SELECT id, status, download_job_id, source_path, previous_task_id, media_type, media_item_id, episode_id, library_id, name_template_id, dest_path, import_method, media_file_id, attempt_count, max_attempts, next_run_at, last_error, error_category, created_at, updated_at FROM import_task
WHERE download_job_id = $1
ORDER BY created_at DESC
`

func (q *Queries) ListImportTasksByDownloadJob(ctx context.Context, downloadJobID pgtype.UUID) ([]ImportTask, error) {
	rows, err := q.db.Query(ctx, listImportTasksByDownloadJob, downloadJobID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []ImportTask
	for rows.Next() {
		var i ImportTask
		if err := rows.Scan(
			&i.ID,
			&i.Status,
			&i.DownloadJobID,
			&i.SourcePath,
			&i.PreviousTaskID,
			&i.MediaType,
			&i.MediaItemID,
			&i.EpisodeID,
			&i.LibraryID,
			&i.NameTemplateID,
			&i.DestPath,
			&i.ImportMethod,
			&i.MediaFileID,
			&i.AttemptCount,
			&i.MaxAttempts,
			&i.NextRunAt,
			&i.LastError,
			&i.ErrorCategory,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const listImportTasksByEpisode = `-- name: ListImportTasksByEpisode :many
SELECT id, status, download_job_id, source_path, previous_task_id, media_type, media_item_id, episode_id, library_id, name_template_id, dest_path, import_method, media_file_id, attempt_count, max_attempts, next_run_at, last_error, error_category, created_at, updated_at FROM import_task
WHERE episode_id = $1
ORDER BY created_at DESC
`

func (q *Queries) ListImportTasksByEpisode(ctx context.Context, episodeID pgtype.UUID) ([]ImportTask, error) {
	rows, err := q.db.Query(ctx, listImportTasksByEpisode, episodeID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []ImportTask
	for rows.Next() {
		var i ImportTask
		if err := rows.Scan(
			&i.ID,
			&i.Status,
			&i.DownloadJobID,
			&i.SourcePath,
			&i.PreviousTaskID,
			&i.MediaType,
			&i.MediaItemID,
			&i.EpisodeID,
			&i.LibraryID,
			&i.NameTemplateID,
			&i.DestPath,
			&i.ImportMethod,
			&i.MediaFileID,
			&i.AttemptCount,
			&i.MaxAttempts,
			&i.NextRunAt,
			&i.LastError,
			&i.ErrorCategory,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const listImportTasksByMediaItem = `-- name: ListImportTasksByMediaItem :many
SELECT id, status, download_job_id, source_path, previous_task_id, media_type, media_item_id, episode_id, library_id, name_template_id, dest_path, import_method, media_file_id, attempt_count, max_attempts, next_run_at, last_error, error_category, created_at, updated_at FROM import_task
WHERE media_item_id = $1
ORDER BY created_at DESC
`

func (q *Queries) ListImportTasksByMediaItem(ctx context.Context, mediaItemID pgtype.UUID) ([]ImportTask, error) {
	rows, err := q.db.Query(ctx, listImportTasksByMediaItem, mediaItemID)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []ImportTask
	for rows.Next() {
		var i ImportTask
		if err := rows.Scan(
			&i.ID,
			&i.Status,
			&i.DownloadJobID,
			&i.SourcePath,
			&i.PreviousTaskID,
			&i.MediaType,
			&i.MediaItemID,
			&i.EpisodeID,
			&i.LibraryID,
			&i.NameTemplateID,
			&i.DestPath,
			&i.ImportMethod,
			&i.MediaFileID,
			&i.AttemptCount,
			&i.MaxAttempts,
			&i.NextRunAt,
			&i.LastError,
			&i.ErrorCategory,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const listImportTasksByStatus = `-- name: ListImportTasksByStatus :many
SELECT id, status, download_job_id, source_path, previous_task_id, media_type, media_item_id, episode_id, library_id, name_template_id, dest_path, import_method, media_file_id, attempt_count, max_attempts, next_run_at, last_error, error_category, created_at, updated_at FROM import_task
WHERE status = $1
ORDER BY created_at DESC
LIMIT $3
OFFSET $2
`

type ListImportTasksByStatusParams struct {
	Status    string `json:"status"`
	OffsetVal int32  `json:"offset_val"`
	LimitVal  int32  `json:"limit_val"`
}

func (q *Queries) ListImportTasksByStatus(ctx context.Context, arg ListImportTasksByStatusParams) ([]ImportTask, error) {
	rows, err := q.db.Query(ctx, listImportTasksByStatus, arg.Status, arg.OffsetVal, arg.LimitVal)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []ImportTask
	for rows.Next() {
		var i ImportTask
		if err := rows.Scan(
			&i.ID,
			&i.Status,
			&i.DownloadJobID,
			&i.SourcePath,
			&i.PreviousTaskID,
			&i.MediaType,
			&i.MediaItemID,
			&i.EpisodeID,
			&i.LibraryID,
			&i.NameTemplateID,
			&i.DestPath,
			&i.ImportMethod,
			&i.MediaFileID,
			&i.AttemptCount,
			&i.MaxAttempts,
			&i.NextRunAt,
			&i.LastError,
			&i.ErrorCategory,
			&i.CreatedAt,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const scheduleImportTaskRetry = `-- name: ScheduleImportTaskRetry :one
UPDATE import_task
SET attempt_count = attempt_count + 1,
    last_error = $1,
    error_category = $2,
    next_run_at = $3,
    updated_at = now()
WHERE id = $4
RETURNING id, status, download_job_id, source_path, previous_task_id, media_type, media_item_id, episode_id, library_id, name_template_id, dest_path, import_method, media_file_id, attempt_count, max_attempts, next_run_at, last_error, error_category, created_at, updated_at
`

type ScheduleImportTaskRetryParams struct {
	LastError     *string     `json:"last_error"`
	ErrorCategory *string     `json:"error_category"`
	NextRunAt     time.Time   `json:"next_run_at"`
	ID            pgtype.UUID `json:"id"`
}

func (q *Queries) ScheduleImportTaskRetry(ctx context.Context, arg ScheduleImportTaskRetryParams) (ImportTask, error) {
	row := q.db.QueryRow(ctx, scheduleImportTaskRetry,
		arg.LastError,
		arg.ErrorCategory,
		arg.NextRunAt,
		arg.ID,
	)
	var i ImportTask
	err := row.Scan(
		&i.ID,
		&i.Status,
		&i.DownloadJobID,
		&i.SourcePath,
		&i.PreviousTaskID,
		&i.MediaType,
		&i.MediaItemID,
		&i.EpisodeID,
		&i.LibraryID,
		&i.NameTemplateID,
		&i.DestPath,
		&i.ImportMethod,
		&i.MediaFileID,
		&i.AttemptCount,
		&i.MaxAttempts,
		&i.NextRunAt,
		&i.LastError,
		&i.ErrorCategory,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const setImportTaskCompleted = `-- name: SetImportTaskCompleted :one
UPDATE import_task
SET status = 'completed',
    dest_path = $1,
    import_method = $2,
    media_file_id = $3,
    updated_at = now()
WHERE id = $4
RETURNING id, status, download_job_id, source_path, previous_task_id, media_type, media_item_id, episode_id, library_id, name_template_id, dest_path, import_method, media_file_id, attempt_count, max_attempts, next_run_at, last_error, error_category, created_at, updated_at
`

type SetImportTaskCompletedParams struct {
	DestPath     *string     `json:"dest_path"`
	ImportMethod *string     `json:"import_method"`
	MediaFileID  pgtype.UUID `json:"media_file_id"`
	ID           pgtype.UUID `json:"id"`
}

func (q *Queries) SetImportTaskCompleted(ctx context.Context, arg SetImportTaskCompletedParams) (ImportTask, error) {
	row := q.db.QueryRow(ctx, setImportTaskCompleted,
		arg.DestPath,
		arg.ImportMethod,
		arg.MediaFileID,
		arg.ID,
	)
	var i ImportTask
	err := row.Scan(
		&i.ID,
		&i.Status,
		&i.DownloadJobID,
		&i.SourcePath,
		&i.PreviousTaskID,
		&i.MediaType,
		&i.MediaItemID,
		&i.EpisodeID,
		&i.LibraryID,
		&i.NameTemplateID,
		&i.DestPath,
		&i.ImportMethod,
		&i.MediaFileID,
		&i.AttemptCount,
		&i.MaxAttempts,
		&i.NextRunAt,
		&i.LastError,
		&i.ErrorCategory,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const setImportTaskFailed = `-- name: SetImportTaskFailed :one
UPDATE import_task
SET status = 'failed',
    last_error = $1,
    error_category = $2,
    updated_at = now()
WHERE id = $3
RETURNING id, status, download_job_id, source_path, previous_task_id, media_type, media_item_id, episode_id, library_id, name_template_id, dest_path, import_method, media_file_id, attempt_count, max_attempts, next_run_at, last_error, error_category, created_at, updated_at
`

type SetImportTaskFailedParams struct {
	LastError     *string     `json:"last_error"`
	ErrorCategory *string     `json:"error_category"`
	ID            pgtype.UUID `json:"id"`
}

func (q *Queries) SetImportTaskFailed(ctx context.Context, arg SetImportTaskFailedParams) (ImportTask, error) {
	row := q.db.QueryRow(ctx, setImportTaskFailed, arg.LastError, arg.ErrorCategory, arg.ID)
	var i ImportTask
	err := row.Scan(
		&i.ID,
		&i.Status,
		&i.DownloadJobID,
		&i.SourcePath,
		&i.PreviousTaskID,
		&i.MediaType,
		&i.MediaItemID,
		&i.EpisodeID,
		&i.LibraryID,
		&i.NameTemplateID,
		&i.DestPath,
		&i.ImportMethod,
		&i.MediaFileID,
		&i.AttemptCount,
		&i.MaxAttempts,
		&i.NextRunAt,
		&i.LastError,
		&i.ErrorCategory,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const setImportTaskInProgress = `-- name: SetImportTaskInProgress :one
UPDATE import_task
SET status = 'in_progress',
    updated_at = now()
WHERE id = $1
RETURNING id, status, download_job_id, source_path, previous_task_id, media_type, media_item_id, episode_id, library_id, name_template_id, dest_path, import_method, media_file_id, attempt_count, max_attempts, next_run_at, last_error, error_category, created_at, updated_at
`

func (q *Queries) SetImportTaskInProgress(ctx context.Context, id pgtype.UUID) (ImportTask, error) {
	row := q.db.QueryRow(ctx, setImportTaskInProgress, id)
	var i ImportTask
	err := row.Scan(
		&i.ID,
		&i.Status,
		&i.DownloadJobID,
		&i.SourcePath,
		&i.PreviousTaskID,
		&i.MediaType,
		&i.MediaItemID,
		&i.EpisodeID,
		&i.LibraryID,
		&i.NameTemplateID,
		&i.DestPath,
		&i.ImportMethod,
		&i.MediaFileID,
		&i.AttemptCount,
		&i.MaxAttempts,
		&i.NextRunAt,
		&i.LastError,
		&i.ErrorCategory,
		&i.CreatedAt,
		&i.UpdatedAt,
	)
	return i, err
}

const updateImportTaskSourcePath = `-- name: UpdateImportTaskSourcePath :exec
UPDATE import_task
SET source_path = $2, updated_at = now()
WHERE id = $1
`

type UpdateImportTaskSourcePathParams struct {
	ID         pgtype.UUID `json:"id"`
	SourcePath string      `json:"source_path"`
}

func (q *Queries) UpdateImportTaskSourcePath(ctx context.Context, arg UpdateImportTaskSourcePathParams) error {
	_, err := q.db.Exec(ctx, updateImportTaskSourcePath, arg.ID, arg.SourcePath)
	return err
}
